{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sqlalchemy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# Define la configuración de la base de datos\n",
    "database_config = {\n",
    "    'server': r'DESKTOP-S9274BN\\SQLEXPRESS',\n",
    "    'database': 'nba_henry',\n",
    "    # 'username': 'your_username',\n",
    "    # 'password': 'your_password'\n",
    "}\n",
    "\n",
    "# Crear la cadena de conexión\n",
    "connection_string = f\"mssql+pyodbc://@{database_config['server']}/{database_config['database']}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "# Crear el motor de conexión\n",
    "engine = create_engine(connection_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler(FileSystemEventHandler):\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        print(\"DataHandler initialized.\")\n",
    "\n",
    "    def on_created(self, event):\n",
    "        print(f\"Event detected: {event}\")\n",
    "        if event.is_directory:\n",
    "            print(\"The created event is a directory. Ignoring.\")\n",
    "            return None\n",
    "        elif event.src_path.endswith(\".csv\"):\n",
    "            print(f\"CSV file detected: {event.src_path}\")\n",
    "            self.process_new_file(event.src_path)\n",
    "\n",
    "    def process_new_file(self, file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Read the new CSV file and drop the 'Unnamed: 0' column if it exists\n",
    "        new_data = pd.read_csv(file_path)\n",
    "\n",
    "        if 'Unnamed: 0' in new_data.columns:\n",
    "            new_data = new_data.drop(columns=['Unnamed: 0'])\n",
    "        # Convertimos las columnas que contengan la palabra date a formato date\n",
    "        for column in new_data.columns:\n",
    "            if 'date' in column.lower():\n",
    "                new_data[column] = pd.to_datetime(new_data[column], errors='coerce')\n",
    "            \n",
    "        print(f\"New data read from {file_path}:\\n{new_data.head()}\")\n",
    "        print('new_data.dtypes:')\n",
    "        print(new_data.dtypes)\n",
    "        \n",
    "\n",
    "        # Load existing data from the database\n",
    "        existing_data = self.load_existing_data(filename)\n",
    "        print(f\"Existing data loaded from database:\\n{existing_data.head()}\")\n",
    "\n",
    "        # Identify new or modified rows\n",
    "        changes = self.get_changes(existing_data, new_data,filename)\n",
    "        print(f\"Changes identified:\\n{changes.head()}\")\n",
    "\n",
    "        # Process and insert the changes into SQL Server\n",
    "        self.insert_data_into_sql(changes,filename)\n",
    "    \n",
    "    def load_existing_data(self, filename):\n",
    "        print(filename + \" load existing data\")\n",
    "        \n",
    "        filename_without_extension = filename.replace(\".csv\", \"\")\n",
    "        \n",
    "        query = f\"SELECT * FROM {filename_without_extension}\"\n",
    "        print(query)\n",
    "        \n",
    "        with self.engine.connect() as connection:\n",
    "            # Obtener la información de la estructura de la tabla\n",
    "            table_info_query = f\"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{filename_without_extension}'\"\n",
    "            table_info = pd.read_sql(table_info_query, connection)\n",
    "            \n",
    "            # Crear un diccionario para mapear columnas a sus tipos de datos\n",
    "            dtype_map = {}\n",
    "            for index, row in table_info.iterrows():\n",
    "                column_name = row['column_name']\n",
    "                data_type = row['data_type']\n",
    "                \n",
    "                if data_type in ['date', 'timestamp']:\n",
    "                    dtype_map[column_name] = 'datetime64[ns]'\n",
    "                elif data_type in ['int', 'bigint', 'smallint']:\n",
    "                    dtype_map[column_name] = 'int64'\n",
    "                elif data_type in ['float', 'double precision']:\n",
    "                    dtype_map[column_name] = 'float64'\n",
    "                else:\n",
    "                    dtype_map[column_name] = 'object'\n",
    "            \n",
    "            print(\"Tipo de datos esperado para cada columna:\", dtype_map)\n",
    "            \n",
    "            # Cargar los datos de la tabla respetando los tipos de datos\n",
    "            existing_data = pd.read_sql(query, connection, dtype=dtype_map)\n",
    "            print('existing_data.dtypes:')\n",
    "            print(existing_data.dtypes)\n",
    "        return existing_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_changes(self, existing_data, new_data, filename):\n",
    "        print(filename + \" get changes\")\n",
    "        merged_data = new_data.merge(existing_data, on='id', how='left', suffixes=('', '_existing'))\n",
    "        print('--------------------------------------------')\n",
    "        print(merged_data)\n",
    "    \n",
    "        print('--------------------------------------------')\n",
    "\n",
    "        # Función para verificar si una fila ha cambiado\n",
    "        def row_changed(row):\n",
    "            print('EVALUACION FILA FILA')\n",
    "            print('----------------------------------------------')\n",
    "\n",
    "            for col in new_data.columns:\n",
    "                \n",
    "                if col != 'id':\n",
    "                    print(new_data.columns)\n",
    "                    if row[col] != row.get(f\"{col}_existing\", None):\n",
    "                        \n",
    "                        #print(row[col])\n",
    "                        print(row.get(f\"{col}\"))\n",
    "                        print(row.get(f\"{col}_existing\"))\n",
    "\n",
    "                        \n",
    "                        print('true')\n",
    "                        return True\n",
    "            return False\n",
    "        print('----------------------------------------------')\n",
    "\n",
    "        # Identificar filas que son nuevas o modificadas\n",
    "        changed_rows = merged_data.apply(row_changed, axis=1)\n",
    "        changed_data = merged_data[changed_rows]\n",
    "        \n",
    "\n",
    "        # Eliminar las columnas '_existing'\n",
    "        changed_data = changed_data[new_data.columns]\n",
    "    \n",
    "        print('get changes')\n",
    "        print(changed_data)\n",
    "        return changed_data                                      \n",
    "\n",
    "\n",
    "    def insert_data_into_sql(self, data,filename):\n",
    "        filename_without_extension = filename.replace(\".csv\", \"\")\n",
    "        \n",
    "        if data.empty:\n",
    "            print(\"No new or modified rows to insert.\")\n",
    "            return\n",
    "        \n",
    "        with self.engine.connect() as connection:\n",
    "            # Obtener los nombres de las columnas\n",
    "            data.to_sql(filename_without_extension, connection, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la función principal para iniciar el observador\n",
    "if __name__ == \"__main__\":\n",
    "    path_to_watch = '/carpeta_watchdog'  # Cambia esto a tu directorio real\n",
    "    print(f\"Starting to watch directory: {path_to_watch}\")\n",
    "    event_handler = DataHandler(engine)\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path=path_to_watch, recursive=False)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)  # Mantén el script en ejecución\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
